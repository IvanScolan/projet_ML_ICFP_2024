{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TBHCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TBHCNN, self).__init__()\n",
    "        self.H_size_added = 0\n",
    "        self.HR = None\n",
    "        self.R = None\n",
    "        self.R_without_opposite = None\n",
    "        self.K = None\n",
    "        self.references = None\n",
    "        self.wholebandstructure = None\n",
    "        self.reproduces = None\n",
    "\n",
    "    def read_training_set(self, references, kvectors):\n",
    "        numk, numb = references.shape\n",
    "        self.numb = numb\n",
    "        self.numk = numk\n",
    "        self.H_size_init = numb\n",
    "        self.H_size = numb\n",
    "        self.references = tf.constant(references, dtype=tf.float64)\n",
    "        self.K = tf.constant(kvectors, dtype=tf.complex64)\n",
    "    \n",
    "    def define_TB_representation(self, rvectors_without_opposite):\n",
    "        len_rvector = rvectors_without_opposite.shape[0]\n",
    "        numr = 2 * len_rvector - 1\n",
    "        self.R = np.array([])\n",
    "        \n",
    "        for i in rvectors_without_opposite:\n",
    "            self.R = np.append(self.R, i)\n",
    "            if np.sum(np.abs(i)) != 0:\n",
    "                self.R = np.append(self.R, -1*i)\n",
    "                \n",
    "        self.numr = numr        \n",
    "        self.R = self.R.reshape(-1, 3)\n",
    "        self.R = tf.cast(self.R, dtype=tf.complex64)\n",
    "        self.R_without_opposite = rvectors_without_opposite\n",
    "\n",
    "    def reinitialize(self):\n",
    "        self.HR = []\n",
    "        self.H_size = self.H_size_init + self.H_size_added\n",
    "        \n",
    "        for i in self.R_without_opposite:\n",
    "            H_tmp = tf.Variable(tf.random.truncated_normal([self.H_size, self.H_size], mean=0.0, stddev=1.0, dtype=tf.float64))\n",
    "            H_tmp = tf.cast(H_tmp, dtype=tf.complex64)\n",
    "            \n",
    "            if np.sum(np.abs(i)) != 0:\n",
    "                self.HR.append(H_tmp)\n",
    "                self.HR.append(tf.transpose(H_tmp))\n",
    "            else:\n",
    "                self.HR.append(H_tmp + tf.transpose(H_tmp))\n",
    "                \n",
    "        self.HR = tf.stack(self.HR)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Assuming 'inputs' will be the k-vectors\n",
    "        return self.compute_bands(inputs)\n",
    "\n",
    "\n",
    "    def compute_bands(self,k_vectors):\n",
    "        reproduces = tf.zeros([self.numk, self.H_size], dtype=tf.float64)\n",
    "        for i in range(self.numk):\n",
    "            HK = tf.zeros([self.H_size, self.H_size], dtype=tf.complex64)\n",
    "            for j in range(self.numr):\n",
    "                HK += tf.scalar_mul(tf.exp(1j * tf.reduce_sum(self.K[i] * self.R[j])), self.HR[j])\n",
    "                \n",
    "            e = tf.linalg.eigvalsh(HK)\n",
    "            e = tf.cast(e, dtype=tf.float64)\n",
    "            e = tf.reshape(e, [1, self.H_size])\n",
    "            reproduces += tf.tensor_scatter_nd_update(reproduces, [[i]], e)\n",
    "         \n",
    "        self.wholebandstructure = reproduces\n",
    "        reproduces = reproduces[:, int(self.H_size_added/2):int(self.H_size_added/2) + self.numb]\n",
    "        self.reproduces = reproduces\n",
    "        \n",
    "        return reproduces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal loss value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_loss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mdefine_TB_representation(np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32))\n\u001b[0;32m     45\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate)\n\u001b[1;32m---> 47\u001b[0m finished \u001b[38;5;241m=\u001b[39m \u001b[43mfitting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_training_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m finished:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Add basis and reinitialize model if needed\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# model.H_size_added += basis_added_step\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     model\u001b[38;5;241m.\u001b[39mreinitialize()\n",
      "Cell \u001b[1;32mIn[23], line 34\u001b[0m, in \u001b[0;36mfitting\u001b[1;34m(model, optimizer, loss_threshold, max_train_steps, references, kvectors)\u001b[0m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_train_steps):\n\u001b[1;32m---> 34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m max_train_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, inputs, outputs, optimizer)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model, inputs, outputs, optimizer):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 24\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure you call the model on the inputs to get the predictions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(predictions \u001b[38;5;241m-\u001b[39m outputs))\n\u001b[0;32m     26\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32mc:\\Users\\scolan_ivan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[22], line 58\u001b[0m, in \u001b[0;36mTBHCNN.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Assuming 'inputs' will be the k-vectors\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bands\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 66\u001b[0m, in \u001b[0;36mTBHCNN.compute_bands\u001b[1;34m(self, k_vectors)\u001b[0m\n\u001b[0;32m     64\u001b[0m HK \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_size], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcomplex64)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumr):\n\u001b[1;32m---> 66\u001b[0m     HK \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mscalar_mul(tf\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK[i] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[j])), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHR\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     68\u001b[0m e \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvalsh(HK)\n\u001b[0;32m     69\u001b[0m e \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(e, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# we take the example of the 13-atom-wide InSe\n",
    "\n",
    "# Define the tight-binding representation by selecting the real-space Hamiltonian considered\n",
    "\"\"\"\n",
    "Here, to ensure that the Hamiltonians with the opposite lattice vectors are always taken into consideration simultaneously, and they tranpose each other\n",
    "only one of each pair of the opposite lattice vectors should be included in the rvectors array, the other one will be handled automatically\n",
    "e.g., for this system, both np.array([[0,0,0],[0,0,1],]) and np.array([[0,0,0],[0,0,-1],]) represent the lattice vector set np.array([[0,0,0],[0,0,1],[0,0,-1]])\n",
    "and in the end we shall get the 3 real-space Hamiltonian matrices we actually used to build the tight-binding model\n",
    "\"\"\"\n",
    "rvectors_without_opposite = np.array([[0,0,0],[0,0,1],], dtype=np.int32) # in units of[a, b, c] (a, b, and c are the real-space basis vectors; [l, n, m] means the lattice vector l*a+n*b+m*c)\n",
    "\n",
    "# Load data\n",
    "references = np.load(\"./data/input/InSe Nanoribbon/InSe-references.npy\")\n",
    "kvectors = np.load(\"./data/input/InSe Nanoribbon/InSe-kpoints.npy\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "loss_threshold = 1e-5\n",
    "max_training_steps = 10000\n",
    "basis_added_step = 2\n",
    "\n",
    "def train_step(model, inputs, outputs, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)  # Ensure you call the model on the inputs to get the predictions\n",
    "        loss = tf.reduce_mean(tf.square(predictions - outputs))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss  # Make sure to return the loss from the function\n",
    "\n",
    "def fitting(model, optimizer, loss_threshold, max_train_steps, references, kvectors):\n",
    "    train_steps = 0\n",
    "    loss = 0\n",
    "    for step in range(max_train_steps):\n",
    "        loss = train_step(model, kvectors, references, optimizer)\n",
    "        if step % 1000 == 0 or step == max_train_steps - 1:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy():.8f}\")\n",
    "        if loss < loss_threshold:\n",
    "            break\n",
    "    return loss < loss_threshold\n",
    "\n",
    "def main():\n",
    "    model = TBHCNN()\n",
    "    model.read_training_set(references, kvectors)\n",
    "    model.define_TB_representation(np.array([[0,0,0],[0,0,1]], dtype=np.int32))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    finished = fitting(model, optimizer, loss_threshold, max_training_steps, references, kvectors)\n",
    "\n",
    "    while not finished:\n",
    "        # Add basis and reinitialize model if needed\n",
    "        # model.H_size_added += basis_added_step\n",
    "        model.reinitialize()\n",
    "        finished = fitting(model, optimizer, loss_threshold, max_training_steps, references, kvectors)\n",
    "\n",
    "    if finished:\n",
    "        Resulting_Hamiltonian = model.HR.numpy()\n",
    "        Rvectors_of_the_resulting_hamiltonian = model.R.numpy().astype(np.int32)\n",
    "        Reproduced_TB_bandstructure = model.wholebandstructure.numpy()\n",
    "        Reproduced_TB_bands = model.reproduces.numpy()\n",
    "\n",
    "        np.save(\"./data/output/InSe Nanoribbon/resulting_real_space_hamiltonians.npy\", Resulting_Hamiltonian)\n",
    "        np.save(\"./data/output/InSe Nanoribbon/rvectors_of_the_resulting_hamiltonian.npy\", Rvectors_of_the_resulting_hamiltonian)\n",
    "        np.save(\"./data/output/InSe Nanoribbon/TB_bandstructure.npy\", Reproduced_TB_bandstructure)\n",
    "        np.save(\"./data/output/InSe Nanoribbon/reproduced_TB_bands.npy\", Reproduced_TB_bands)\n",
    "\n",
    "        # Calculate and print the final loss value\n",
    "        final_loss_value = tf.reduce_mean(tf.square(model.reproduces - model.references)).numpy()\n",
    "        print(f\"final loss value: {final_loss_value:.8f}\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
